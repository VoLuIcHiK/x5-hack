import numpy as np
import pandas as pd
import random
import re
import json
import asyncio
import pymorphy3
from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, pipeline
from transformers import BertConfig, BertForTokenClassification, BertTokenizerFast, pipeline
from sentence_transformers import SentenceTransformer
from datasets import Dataset
from sklearn.metrics.pairwise import cosine_similarity
import torch
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import normalize
from fuzzywuzzy import process
from sklearn.cluster import KMeans
from sklearn.metrics import f1_score, precision_score, recall_score
from ast import literal_eval
import evaluate
from functools import partial
from pydantic import BaseModel
from typing import List
import os

# –ö–ª–∞—Å—Å –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ FastAPI
class EntityResult(BaseModel):
    start_index: int
    end_index: int
    entity: str
    text: str = ""

class SoftValidationParser:
    def __init__(self):
        # —à–∞–±–ª–æ–Ω—ã –µ–¥–∏–Ω–∏—Ü
        self.volume_units = [
            '–≥', r'–≥—Ä\.?', r'–≥—Ä–∞–º–º\.?',
            '–∫–≥', r'–∫–∏–ª–æ–≥—Ä–∞–º–º\.?',
            '–º–ª', r'–º–ª\.?', r'–º–∏–ª–ª–∏–ª–∏—Ç—Ä\.?',
            '–ª', r'–ª\.?', r'–ª–∏—Ç—Ä\.?',
            r'—à—Ç\.?', r'—à—Ç—É–∫\.?'
        ]
        
        self.percent_units = ['%', r'–ø—Ä–æ—Ü–µ–Ω—Ç(?:–æ–≤)?', r'–ø—Ä–æ—Ü\.?', r'–∂–∏—Ä–Ω\.?']
        
        # –∫–æ–º–±–∏–Ω–∏—Ä—É–µ–º –µ–¥–∏–Ω–∏—Ü—ã –≤ –æ–¥–∏–Ω —à–∞–±–ª–æ–Ω
        self.vol_pattern = re.compile(f'(\d+[.,]?\d*)\s*({"|".join(self.volume_units)})',
            flags=re.IGNORECASE
        )
        
        self.pct_pattern = re.compile(
            rf'(\d+[.,]?\d*)\s*({"|".join(self.percent_units)})',
            flags=re.IGNORECASE
        )

    def preprocess_volume_text(self, text):
        """–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
        replacements = {
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã
            r'(\d+(?:[.,]\d+)?)\s*([–∞-—è–ê-–Ø%]+)': r'\1 \2',
            r'(\d+(?:[.,]\d+)?)\s{2,}([–∞-—è–ê-–Ø%]+)': r'\1 \2',
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä—É–µ–º –¥–µ—Å—è—Ç–∏—á–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
            r'(\d+),(\d+)': r'\1.\2',
            # –ï–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è
            r'\b(–≥—Ä–∞–º–º?|–≥—Ä\.?)\b': '–≥',
            r'\b(–∫–∏–ª–æ–≥—Ä–∞–º–º?|–∫–∏–ª–æ|–∫–≥\.?)\b': '–∫–≥',
            r'\b(–º–∏–ª–ª–∏–ª–∏—Ç—Ä?|–º–ª\.?)\b': '–º–ª',
            r'\b(–ª–∏—Ç—Ä?|–ª\.?)\b': '–ª',
            r'\b(—à—Ç—É–∫[–∞–∏]?|—à—Ç\.?)\b': '—à—Ç',
            # –¢–µ–∫—Å—Ç–æ–≤—ã–µ —á–∏—Å–ª–∞
            r'\b–ø–æ–ª\b': '0.5',
            r'\b–ø–æ–ª—Ç–æ—Ä–∞\b': '1.5',
            r'\b–ø–æ–ª–æ–≤–∏–Ω[–∞—É]\b': '0.5',
            r'\b—á–µ—Ç–≤–µ—Ä—Ç—å\b': '0.25',
            # –ü—Ä–æ—Ü–µ–Ω—Ç—ã –∏ –∫—Ä–µ–ø–æ—Å—Ç—å
            r'\b(\d+(?:[.,]\d+)?)\s*%\b': r'\1 %',
            r'\b(\d+(?:[.,]\d+)?)\s*(–ø—Ä–æ—Ü–µ–Ω—Ç(?:–æ–≤)?|–ø—Ä–æ—Ü\.?|–∂–∏—Ä–Ω\.?|–∫—Ä–µ–ø–æ—Å—Ç(?:—å)?|–≥—Ä–∞–¥—É—Å(?:–æ–≤)?)\b': r'\1 %',
        }
        
        result = text
        for pattern, repl in replacements.items():
            result = re.sub(pattern, repl, result, flags=re.IGNORECASE)
        return result

    def parse_with_soft_validation(self, text, type_entity=None):
        # 1. –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
        normalized = self.preprocess_volume_text(text)
        entities = []

        # 2. –ü–æ–∏—Å–∫ –æ–±—ä—ë–º–æ–≤
        for m in self.vol_pattern.finditer(normalized):
            start, end = m.span()
            span = normalized[start:end].split()
            offset = start
            for i, part in enumerate(span):
                p_start = normalized.find(part, offset)
                p_end = p_start + len(part)
                tag = 'B-VOLUME' if i == 0 else 'I-VOLUME'
                entities.append((p_start, p_end, tag))
                offset = p_end

        # 3. –ü–æ–∏—Å–∫ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤
        for m in self.pct_pattern.finditer(normalized):
            start, end = m.span()
            span = normalized[start:end].split()
            offset = start
            for i, part in enumerate(span):
                p_start = normalized.find(part, offset)
                p_end = p_start + len(part)
                tag = 'B-PERCENT' if i == 0 else 'I-PERCENT'
                entities.append((p_start, p_end, tag))
                offset = p_end

        entities.sort(key=lambda x: x[0])
        return entities

# –û—Å–Ω–æ–≤–Ω–æ–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä
class ImprovedProductEntityExtractor:
    def __init__(self,
                 ner_model_path="./model_files",
                 embedding_model_name="cointegrated/rubert-tiny2",
                 catalog_brands=None,
                 catalog_categories=None):
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ NER –º–æ–¥–µ–ª–∏ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
        try:
            if ner_model_path and os.path.exists(ner_model_path):
                print(f"Loading custom NER model from: {ner_model_path}")
                self.tokenizer_ner = AutoTokenizer.from_pretrained(ner_model_path, is_split_into_words=True)
                self.model_ner = AutoModelForTokenClassification.from_pretrained(ner_model_path)
                
                # Pipeline NER —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∞–≥—Ä–µ–≥–∞—Ü–∏–µ–π
                self.pipeline_ner = pipeline(
                    "ner",
                    model=self.model_ner,
                    tokenizer=self.tokenizer_ner,
                    aggregation_strategy="simple",
                    device=-1
                )
                print("‚úÖ Custom NER model loaded successfully!")
            else:
                print(f"‚ùå NER model path not found: {ner_model_path}")
                self.tokenizer_ner = None
                self.model_ner = None
                self.pipeline_ner = None
                
        except Exception as e:
            print(f"‚ùå Error loading NER model from {ner_model_path}: {e}")
            self.tokenizer_ner = None
            self.model_ner = None
            self.pipeline_ner = None

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è embedding –º–æ–¥–µ–ª–∏
        try:
            if embedding_model_name:
                print(f"Loading embedding model: {embedding_model_name}")
                self.embedding_model = SentenceTransformer(embedding_model_name)
                print("‚úÖ Embedding model loaded successfully!")
            else:
                self.embedding_model = None
        except Exception as e:
            print(f"‚ùå Warning: Could not load embedding model {embedding_model_name}: {e}")
            self.embedding_model = None

        self.catalog_brands = catalog_brands or []
        self.catalog_categories = catalog_categories or []
        self.morph = pymorphy3.MorphAnalyzer()
        
        self.id2label = {
            0: 'O',
            1: 'B-TYPE',
            2: 'I-TYPE',
            3: 'B-BRAND',
            4: 'I-BRAND',
            5: 'B-VOLUME',
            6: 'I-VOLUME',
            7: 'B-PERCENT',
            8: 'I-PERCENT'
        }
        
        # –ü–∞—Ä—Å–µ—Ä –æ–±—ä–µ–º–æ–≤
        self.soft_parser = SoftValidationParser()

    def lemmatize(self, word):
        """–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ pymorphy3"""
        if not word or not isinstance(word, str):
            return word or ""
        try:
            parsed = self.morph.parse(word)
            return parsed[0].normal_form if parsed else word.lower()
        except:
            return word.lower()

    def predict(self, text: str):
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è, –≤–æ–∑–≤—Ä–∞—â–∞—é—â–∏–π BIO-–º–µ—Ç–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞.
        """
        # 1. –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞ –∏ —Å—á–∏—Ç–∞–µ–º –∏—Ö spans
        words = text.split()
        spans = []
        pos = 0
        for w in words:
            spans.append((pos, pos + len(w)))
            pos += len(w) + 1

        # 2. –ü–æ–ª—É—á–∞–µ–º —Å—ã—Ä—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        ner_results = []
        if self.pipeline_ner:
            try:
                ner_results = list(self.pipeline_ner(text, aggregation_strategy="simple"))
            except Exception as e:
                print(f"Error in NER pipeline: {e}")
                
        soft_results = self.soft_parser.parse_with_soft_validation(text)

        # 3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≥—Ä—É–ø–ø—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞
        word_groups = [None] * len(spans)

        # 4. –ü—Ä–∏–º–µ–Ω—è–µ–º soft-parser (–æ–Ω–∏ —É–∂–µ —Å–æ–¥–µ—Ä–∂–∞—Ç B-/I- –º–µ—Ç–∫–∏, –±–µ—Ä—ë–º –ø—Ä–æ—Å—Ç–æ –≥—Ä—É–ø–ø—É)
        for start, end, label in soft_results:
            grp = label.split('-', 1)[1]
            for idx, (w_start, w_end) in enumerate(spans):
                if start < w_end and end > w_start:
                    word_groups[idx] = grp

        # 5. –ü—Ä–∏–º–µ–Ω—è–µ–º NER —Ç—É–¥–∞, –≥–¥–µ –µ—â—ë –Ω–µ—Ç soft-–º–µ—Ç–∫–∏
        for r in ner_results:
            grp = r["entity_group"].upper()
            for idx, (w_start, w_end) in enumerate(spans):
                if word_groups[idx] is None and r["start"] < w_end and r["end"] > w_start:
                    word_groups[idx] = grp

        # 6. –°—Ç—Ä–æ–∏–º BIO-–º–µ—Ç–∫–∏ –ø–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≥—Ä—É–ø–ø
        bio_tags = []
        prev_group = None
        for grp in word_groups:
            if grp is None:
                bio_tags.append('O')
                prev_group = None
            else:
                if grp == prev_group:
                    bio_tags.append(f'I-{grp}')
                else:
                    bio_tags.append(f'B-{grp}')
                prev_group = grp

        # 7. –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ, —Å–æ–≤–º–µ—Å—Ç–∏–º–æ–º —Å FastAPI
        entities = []
        for i, (start, end) in enumerate(spans):
            if bio_tags[i] != 'O':
                entities.append({
                    'start_index': start,
                    'end_index': end,
                    'entity': bio_tags[i],
                    'text': text[start:end]
                })
        
        return entities

    def extract_entities_with_soft_validation(self, query):
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π —Å—É—â–Ω–æ—Å—Ç–µ–π"""
        entities = self.predict(query)
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å—É—â–Ω–æ—Å—Ç–∏ –ø–æ —Ç–∏–ø–∞–º
        type_entities = []
        brand_entities = []
        volume_entities = []
        percent_entities = []
        
        for entity in entities:
            entity_type = entity['entity']
            if 'TYPE' in entity_type:
                type_entities.append(entity)
            elif 'BRAND' in entity_type:
                brand_entities.append(entity)
            elif 'VOLUME' in entity_type:
                volume_entities.append(entity)
            elif 'PERCENT' in entity_type:
                percent_entities.append(entity)
        
        return {
            "original_query": query,
            "corrected_query": query,  # –í –±—É–¥—É—â–µ–º –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∫–æ—Ä—Ä–µ–∫—Ü–∏—é
            "annotations": [(e['start_index'], e['end_index'], e['entity']) for e in entities],
            "type": type_entities,
            "brand": brand_entities,
            "volumes": volume_entities,
            "percents": percent_entities
        }

# –û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å NERModel –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å FastAPI
class NERModel:
    def __init__(self, model_dir: str = "./model_files"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NER –º–æ–¥–µ–ª–∏
        Args:
            model_dir: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –º–æ–¥–µ–ª—å—é
        """
        self.model_dir = model_dir
        print(f"üöÄ Initializing Enhanced NER Service...")
        print(f"üìÇ Model directory: {model_dir}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∏ —Ñ–∞–π–ª–æ–≤
        if os.path.exists(model_dir):
            print(f"‚úÖ Model directory found: {model_dir}")
        else:
            print(f"‚ùå Model directory not found: {model_dir}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä
        try:
            self.extractor = ImprovedProductEntityExtractor(
                ner_model_path=model_dir,
                embedding_model_name="cointegrated/rubert-tiny2"
            )
            print("üéâ Enhanced NER Model initialized successfully!")
        except Exception as e:
            print(f"‚ùå Error initializing extractor: {e}")
            # Fallback –±–µ–∑ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
            self.extractor = ImprovedProductEntityExtractor(
                ner_model_path=None,
                embedding_model_name="cointegrated/rubert-tiny2"
            )

    async def predict(self, text: str) -> List[EntityResult]:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        Args:
            text: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ EntityResult
        """
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
        loop = asyncio.get_event_loop()
        entities = await loop.run_in_executor(None, self._sync_predict, text)
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç EntityResult
        results = []
        for entity in entities:
            results.append(EntityResult(
                start_index=entity['start_index'],
                end_index=entity['end_index'],
                entity=entity['entity'],
                text=entity.get('text', text[entity['start_index']:entity['end_index']])
            ))
        
        return results

    def _sync_predict(self, text: str):
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤ executor"""
        return self.extractor.predict(text)

    def get_detailed_analysis(self, text: str):
        """–ü–æ–ª—É—á–∏—Ç—å –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π"""
        return self.extractor.extract_entities_with_soft_validation(text)

    def info(self):
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        model_exists = os.path.exists(self.model_dir) if self.model_dir else False
        has_custom_model = self.extractor.pipeline_ner is not None
        has_embedding_model = self.extractor.embedding_model is not None
        
        return {
            "model_dir": self.model_dir,
            "status": "loaded",
            "custom_model_loaded": has_custom_model,
            "embedding_model_loaded": has_embedding_model,
            "model_directory_exists": model_exists,
            "entities": ["TYPE", "BRAND", "VOLUME", "PERCENT"],
            "features": [
                "custom_ner_model",
                "soft_validation_parser",
                "embedding_similarity",
                "postprocessing",
                "lemmatization"
            ],
            "fallback_methods": ["soft_parser", "rule_based"]
        }